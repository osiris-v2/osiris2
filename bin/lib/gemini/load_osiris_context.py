import json
import os
import glob
from datetime import datetime

import lib.core as core
core.dynmodule("lib.serializes","serialize")
print(core.serialize.INFO)


# --- Definici√≥n de Marcadores para el Contexto ---
MARKERS = {
    "human_start": "--- INICIO INSTRUCCION HUMANA ---\n",
    "human_end": "\n--- FIN INSTRUCCION HUMANA ---\n",
    "ai_instruction_start": "--- INICIO INSTRUCCION IA ---\n",
    "ai_instruction_end": "\n--- FIN INSTRUCCION IA ---\n",
    "metadata_start": "--- INICIO METADATA ---\n",
    "metadata_end": "\n--- FIN METADATA ---\n",
    "file_start": "--- INICIO ARCHIVO: {} ---\n",
    "file_end": "\n--- FIN ARCHIVO: {} ---\n",
    "path_info_start": "--- INICIO INFO DE PATH: {} ---\n",
    "path_info_end": "\n--- FIN INFO DE PATH ---\n"
}

# --- Estimaci√≥n de Tokens (Aproximada) ---
# Usamos una heur√≠stica simple: 1 token ~ 4 caracteres para texto en ingl√©s/espa√±ol.
def _estimate_tokens(text):
    return len(text) // 4

# --- Funci√≥n para Resolver Rutas Absolutas ---
def _resolve_path(base, relative_path):
    if not os.path.isabs(relative_path):
        return os.path.abspath(os.path.join(base, relative_path))
    return os.path.abspath(relative_path)

# --- Funci√≥n para A√±adir Contenido al Contexto con Gesti√≥n de Tokens y Truncamiento ---
def _add_to_context_string(
    context_builder,
    content_to_add,
    marker_start,
    marker_end,
    current_tokens_count,
    max_tokens_limit,
    warnings_list,
    identifier=""
):
    """
    A√±ade contenido a la cadena del contexto, gestionando l√≠mites de tokens y truncamiento.
    Retorna (nueva_cadena_contexto, nuevos_tokens_totales).
    """
    if not content_to_add:
        return context_builder, current_tokens_count

    # Los marcadores siempre se cuentan y se priorizan.
    total_markers_tokens = _estimate_tokens(marker_start + marker_end)

    if current_tokens_count + total_markers_tokens >= max_tokens_limit:
        warnings_list.append(
            f"Advertencia: No se pudo a√±adir '{identifier}' (marcadores) sin exceder el l√≠mite de tokens "
            f"({current_tokens_count}/{max_tokens_limit} tokens). Elemento omitido. ‚úÇÔ∏è"
        )
        return context_builder, current_tokens_count

    content_tokens = _estimate_tokens(content_to_add)

    # Si el contenido completo m√°s marcadores excede el l√≠mite
    if current_tokens_count + total_markers_tokens + content_tokens > max_tokens_limit:
        remaining_tokens_for_content = max_tokens_limit - current_tokens_count - total_markers_tokens
        
        # Calcular cu√°ntos caracteres podemos permitir para el contenido truncado
        # (Esto es una aproximaci√≥n, puede no ser exacto al token)
        truncate_chars = remaining_tokens_for_content * 4 
        
        if truncate_chars <= 0: # Ni siquiera cabe una peque√±a porci√≥n del contenido
            warnings_list.append(
                f"Advertencia: El elemento '{identifier}' fue omitido porque excede el l√≠mite de tokens "
                f"({current_tokens_count}/{max_tokens_limit} tokens disponibles). Considera refinar tus filtros. ‚úÇÔ∏è"
            )
            return context_builder, current_tokens_count
            
        truncated_content = content_to_add[:truncate_chars]
        
        # Ajustar para asegurar que los marcadores caben al menos
        if len(truncated_content) < len(content_to_add):
            warnings_list.append(
                f"Advertencia: El elemento '{identifier}' fue truncado para no exceder el l√≠mite de tokens "
                f"({current_tokens_count + total_markers_tokens + _estimate_tokens(truncated_content)}/{max_tokens_limit} tokens). Considera refinar tus filtros. ‚úÇÔ∏è"
            )
        
        context_builder.append(marker_start)
        context_builder.append(truncated_content)
        context_builder.append(marker_end)
        return context_builder, current_tokens_count + total_markers_tokens + _estimate_tokens(truncated_content)

    # Si todo cabe
    context_builder.append(marker_start)
    context_builder.append(content_to_add)
    context_builder.append(marker_end)
    return context_builder, current_tokens_count + total_markers_tokens + content_tokens

# --- Funci√≥n Principal: load_osiris_context ---
def load_osiris_context(json_paths, global_base_dir=None):
    """
    Interpreta uno o m√°s archivos .dev.ai.json para construir una cadena de contexto para Gemini AI.

    Args:
        json_paths (list): Una lista de rutas a archivos .dev.ai.json.
                           Si contiene "--help", muestra la ayuda.
        global_base_dir (str, optional): Directorio base global para resolver rutas relativas.
                                         Si None, usa el directorio del archivo JSON actual.

    Returns:
        tuple: (final_context_string, warnings_list)
               final_context_string (str): La cadena de contexto concatenada.
               warnings_list (list): Lista de advertencias generadas durante el procesamiento.
    """
    if json_paths == ["--help"]:
        _print_help_message()
        return "", []

    final_context_parts = [] # Usamos una lista para construir la cadena eficientemente
    warnings = []
    processed_file_paths = set() # Para deduplicaci√≥n de rutas absolutas
    current_total_tokens = 0
    max_tokens_limit = 1000000 # L√≠mite por defecto (1 mill√≥n de tokens)

    # Definimos el orden de las claves para la concatenaci√≥n dentro de un bloque
    # Esto asume que el parser JSON preserva el orden de inserci√≥n, o que estas son las claves esperadas.
    # json.load en Python 3.7+ preserva el orden.
    KEY_ORDER = [
        "maxcontexttokens", # Se procesa primero para establecer el l√≠mite
        "fileencoding",
        "human",
        "aiinstruction",
        "metadata",
        "readfile",
        "readdirectoryfiles",
        "readdirectoryfilesrecursive",
        "readdirectorypaths",
        "readdirectorypathrecursive",
        "filterincludeextensions", # Estos son filtros, no contenido directo, se procesan junto con las claves de directorio
        "filterexcludepatterns",   # Se procesan junto con las claves de directorio
        "responseformat" # Tambi√©n una directiva, no contenido directo para el contexto.
    ]

    for json_file_path in json_paths:
        resolved_json_path = _resolve_path(global_base_dir or os.getcwd(), json_file_path)
        
        if not os.path.exists(resolved_json_path):
            warnings.append(f"Error: Archivo JSON '{resolved_json_path}' no encontrado. Saltando. ‚ùå")
            continue
        if not os.path.isfile(resolved_json_path):
            warnings.append(f"Error: La ruta '{resolved_json_path}' no es un archivo JSON v√°lido. Saltando. ‚ùå")
            continue

        local_base_dir = os.path.dirname(resolved_json_path) if global_base_dir is None else global_base_dir

        try:
            with open(resolved_json_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)

            if not isinstance(json_data, list):
                warnings.append(f"Error: El archivo '{resolved_json_path}' no es un arreglo JSON. Saltando. ‚ùå")
                continue

            for block_idx, block in enumerate(json_data):
                if not isinstance(block, dict):
                    warnings.append(f"Advertencia: El elemento {block_idx} en '{json_file_path}' no es un objeto JSON. Saltando. ‚ö†Ô∏è")
                    continue

                # Normalizar claves a min√∫sculas para procesamiento no case-sensitive
                normalized_block = {k.lower(): v for k, v in block.items()}

                # Procesar maxContextTokens primero si est√° presente para este archivo
                if "maxcontexttokens" in normalized_block and isinstance(normalized_block["maxcontexttokens"], int):
                    max_tokens_limit = normalized_block["maxcontexttokens"]
                    warnings.append(f"Info: L√≠mite de tokens para este contexto establecido en {max_tokens_limit} por '{json_file_path}'.")
                
                block_file_encoding = normalized_block.get("fileencoding", "utf-8")


                # Procesar claves en el orden definido
                for key in KEY_ORDER:
                    if key not in normalized_block:
                        continue
                    
                    value = normalized_block[key]
                    
                    # --- HUMAN ---
                    if key == "human":
                        if isinstance(value, str):
                            final_context_parts, current_total_tokens = _add_to_context_string(
                                final_context_parts, value, MARKERS["human_start"], MARKERS["human_end"],
                                current_total_tokens, max_tokens_limit, warnings, f"instrucci√≥n human en bloque {block_idx}"
                            )
                        else:
                            warnings.append(f"Advertencia: 'human' en bloque {block_idx} debe ser un string. Ignorado. ‚ö†Ô∏è")

                    # --- AI_INSTRUCTION ---
                    elif key == "aiinstruction":
                        if isinstance(value, str):
                            final_context_parts, current_total_tokens = _add_to_context_string(
                                final_context_parts, value, MARKERS["ai_instruction_start"], MARKERS["ai_instruction_end"],
                                current_total_tokens, max_tokens_limit, warnings, f"instrucci√≥n AI en bloque {block_idx}"
                            )
                        else:
                            warnings.append(f"Advertencia: 'aiInstruction' en bloque {block_idx} debe ser un string. Ignorado. ‚ö†Ô∏è")

                    # --- METADATA ---
                    elif key == "metadata":
                        if isinstance(value, dict):
                            metadata_str = json.dumps(value, indent=2, ensure_ascii=False)
                            final_context_parts, current_total_tokens = _add_to_context_string(
                                final_context_parts, metadata_str, MARKERS["metadata_start"], MARKERS["metadata_end"],
                                current_total_tokens, max_tokens_limit, warnings, f"metadata en bloque {block_idx}"
                            )
                        else:
                            warnings.append(f"Advertencia: 'metadata' en bloque {block_idx} debe ser un objeto JSON. Ignorado. ‚ö†Ô∏è")

                    # --- READFILE ---
                    elif key == "readfile":
                        if isinstance(value, list):
                            for file_rel_path in value:
                                if not isinstance(file_rel_path, str):
                                    warnings.append(f"Advertencia: Elemento no string en 'readFile' de bloque {block_idx}. Ignorado. ‚ö†Ô∏è")
                                    continue
                                resolved_file_path = _resolve_path(local_base_dir, file_rel_path)
                                if resolved_file_path in processed_file_paths:
                                    warnings.append(f"Advertencia: El archivo '{resolved_file_path}' ya fue incluido y se omiti√≥ una relectura duplicada para optimizar el contexto. ‚ö†Ô∏è")
                                    continue
                                
                                try:
                                    with open(resolved_file_path, 'r', encoding=block_file_encoding) as f:
                                        file_content = f.read()
                                    final_context_parts, current_total_tokens = _add_to_context_string(
                                        final_context_parts, file_content,
                                        MARKERS["file_start"].format(resolved_file_path),
                                        MARKERS["file_end"].format(resolved_file_path),
                                        current_total_tokens, max_tokens_limit, warnings, f"archivo '{resolved_file_path}'"
                                    )
                                    processed_file_paths.add(resolved_file_path)
                                except FileNotFoundError:
                                    warnings.append(f"fileRead Path '{resolved_file_path}' Not found y contin√∫a la interpretaci√≥n.")
                                except Exception as e:
                                    warnings.append(f"Error al leer archivo '{resolved_file_path}': {e}. Contin√∫a la interpretaci√≥n.")
                        else:
                            warnings.append(f"Advertencia: 'readFile' en bloque {block_idx} debe ser un arreglo de strings. Ignorado. ‚ö†Ô∏è")

                    # --- READ_DIRECTORY_FILES / READ_DIRECTORY_FILES_RECURSIVE ---
                    elif key in ["readdirectoryfiles", "readdirectoryfilesrecursive"]:
                        if not isinstance(value, str):
                            warnings.append(f"Advertencia: '{key}' en bloque {block_idx} debe ser un string con la ruta a un directorio. Ignorado. ‚ö†Ô∏è")
                            continue

                        dir_rel_path = value
                        resolved_dir_path = _resolve_path(local_base_dir, dir_rel_path)
                        
                        if not os.path.isdir(resolved_dir_path):
                            warnings.append(f"DirectoryRead Path '{resolved_dir_path}' Not found o inaccesible y contin√∫a la interpretaci√≥n.")
                            continue

                        is_recursive = (key == "readdirectoryfilesrecursive")
                        
                        include_exts = normalized_block.get("filterincludeextensions", [])
                        if not isinstance(include_exts, list):
                            warnings.append(f"Advertencia: 'filterIncludeExtensions' en bloque {block_idx} debe ser un arreglo de strings. Se ignorar√° el filtro. ‚ö†Ô∏è")
                            include_exts = []
                        
                        exclude_patterns = normalized_block.get("filterexcludepatterns", [])
                        if not isinstance(exclude_patterns, list):
                            warnings.append(f"Advertencia: 'filterExcludePatterns' en bloque {block_idx} debe ser un arreglo de strings. Se ignorar√° el filtro. ‚ö†Ô∏è")
                            exclude_patterns = []

                        if not include_exts and "filterincludeextensions" in normalized_block and not is_recursive and not os.path.isdir(resolved_dir_path):
                            # Esta es la condici√≥n para el aviso de 'includeExtensions' sin directorio v√°lido asociado.
                            # Puede ser compleja de pinpoint, pero intentamos.
                            # Para un directorio v√°lido, arreglo vac√≠o significa 'todas las extensiones'.
                            warnings.append(f"Advertencia: La clave 'filterIncludeExtensions' en el bloque {block_idx} fue ignorada por falta de una clave de directorio v√°lida asociada. ‚ö†Ô∏è")

                        for root, _, files in os.walk(resolved_dir_path):
                            if not is_recursive and root != resolved_dir_path:
                                continue # Solo el nivel superior si no es recursivo

                            for file_name in files:
                                file_path = os.path.join(root, file_name)
                                resolved_file_path = os.path.abspath(file_path) # Asegurar ruta absoluta
                                
                                # Aplicar filtros
                                file_ext = os.path.splitext(file_name)[1].lower()
                                
                                # Exclusi√≥n tiene prioridad
                                if any(glob.fnmatch.fnmatch(resolved_file_path, _resolve_path(local_base_dir, p)) for p in exclude_patterns):
                                    #warnings.append(f"Info: Archivo '{resolved_file_path}' excluido por patr√≥n. üö´")
                                    continue

                                # Inclusi√≥n de extensiones
                                if include_exts and file_ext not in [ext.lower() for ext in include_exts]:
                                    #warnings.append(f"Info: Archivo '{resolved_file_path}' excluido por extensi√≥n. üö´")
                                    continue
                                    
                                if resolved_file_path in processed_file_paths:
                                    warnings.append(f"Advertencia: El archivo '{resolved_file_path}' ya fue incluido y se omiti√≥ una relectura duplicada para optimizar el contexto. ‚ö†Ô∏è")
                                    continue

                                try:
                                    with open(resolved_file_path, 'r', encoding=block_file_encoding) as f:
                                        file_content = f.read()
                                    final_context_parts, current_total_tokens = _add_to_context_string(
                                        final_context_parts, file_content,
                                        MARKERS["file_start"].format(resolved_file_path),
                                        MARKERS["file_end"].format(resolved_file_path),
                                        current_total_tokens, max_tokens_limit, warnings, f"archivo '{resolved_file_path}'"
                                    )
                                    processed_file_paths.add(resolved_file_path)
                                except Exception as e:
                                    warnings.append(f"Error al leer archivo '{resolved_file_path}' desde directorio '{resolved_dir_path}': {e}. Contin√∫a.")

                    # --- READ_DIRECTORY_PATHS / READ_DIRECTORY_PATH_RECURSIVE ---
                    elif key in ["readdirectorypaths", "readdirectorypathrecursive"]:
                        if not isinstance(value, list):
                            warnings.append(f"Advertencia: '{key}' en bloque {block_idx} debe ser un arreglo de strings con rutas a directorios. Ignorado. ‚ö†Ô∏è")
                            continue
                        
                        is_recursive_path = (key == "readdirectorypathrecursive")

                        include_exts = normalized_block.get("filterincludeextensions", [])
                        if not isinstance(include_exts, list):
                            warnings.append(f"Advertencia: 'filterIncludeExtensions' en bloque {block_idx} debe ser un arreglo de strings. Se ignorar√° el filtro. ‚ö†Ô∏è")
                            include_exts = []
                        
                        exclude_patterns = normalized_block.get("filterexcludepatterns", [])
                        if not isinstance(exclude_patterns, list):
                            warnings.append(f"Advertencia: 'filterExcludePatterns' en bloque {block_idx} debe ser un arreglo de strings. Se ignorar√° el filtro. ‚ö†Ô∏è")
                            exclude_patterns = []

                        for dir_rel_path in value:
                            if not isinstance(dir_rel_path, str):
                                warnings.append(f"Advertencia: Elemento no string en '{key}' de bloque {block_idx}. Ignorado. ‚ö†Ô∏è")
                                continue

                            resolved_dir_path = _resolve_path(local_base_dir, dir_rel_path)
                            
                            if not os.path.isdir(resolved_dir_path):
                                warnings.append(f"DirectoryRead Path '{resolved_dir_path}' Not found o inaccesible y contin√∫a la interpretaci√≥n.")
                                continue

                            for root, dirs, files in os.walk(resolved_dir_path):
                                if not is_recursive_path and root != resolved_dir_path:
                                    continue # Solo el nivel superior si no es recursivo para paths
                                
                                current_base_for_relative_path = resolved_dir_path # Para calcular la ruta relativa

                                # Recolectar metadata de directorios
                                if root != resolved_dir_path and is_recursive_path: # Incluir subdirectorios si es recursivo
                                    dir_path_abs = os.path.abspath(root)
                                    if dir_path_abs not in processed_file_paths: # deduplicaci√≥n de directorios
                                        metadata_content = _format_path_metadata(
                                            dir_path_abs,
                                            is_directory=True,
                                            base_path_for_relative=current_base_for_relative_path
                                        )
                                        final_context_parts, current_total_tokens = _add_to_context_string(
                                            final_context_parts, metadata_content,
                                            MARKERS["path_info_start"].format(dir_path_abs),
                                            MARKERS["path_info_end"].format(dir_path_abs),
                                            current_total_tokens, max_tokens_limit, warnings, f"info de path directorio '{dir_path_abs}'"
                                        )
                                        processed_file_paths.add(dir_path_abs)

                                # Recolectar metadata de archivos
                                for file_name in files:
                                    file_path = os.path.join(root, file_name)
                                    resolved_file_path = os.path.abspath(file_path)

                                    # Aplicar filtros
                                    file_ext = os.path.splitext(file_name)[1].lower()
                                    
                                    if any(glob.fnmatch.fnmatch(resolved_file_path, _resolve_path(local_base_dir, p)) for p in exclude_patterns):
                                        continue

                                    if include_exts and file_ext not in [ext.lower() for ext in include_exts]:
                                        continue

                                    if resolved_file_path in processed_file_paths:
                                        warnings.append(f"Advertencia: El path '{resolved_file_path}' ya fue incluido (metadata) y se omiti√≥ una relectura duplicada para optimizar el contexto. ‚ö†Ô∏è")
                                        continue

                                    metadata_content = _format_path_metadata(
                                        resolved_file_path,
                                        is_directory=False,
                                        base_path_for_relative=current_base_for_relative_path
                                    )
                                    final_context_parts, current_total_tokens = _add_to_context_string(
                                        final_context_parts, metadata_content,
                                        MARKERS["path_info_start"].format(resolved_file_path),
                                        MARKERS["path_info_end"].format(resolved_file_path),
                                        current_total_tokens, max_tokens_limit, warnings, f"info de path archivo '{resolved_file_path}'"
                                    )
                                    processed_file_paths.add(resolved_file_path)
                    
                    # --- FILTROS SIN DIRECTORIO ASOCIADO ---
                    # Esto maneja la advertencia si filterIncludeExtensions est√° solo
                    elif key == "filterincludeextensions" and not any(k in normalized_block for k in ["readdirectoryfiles", "readdirectoryfilesrecursive", "readdirectorypaths", "readdirectorypathrecursive"]):
                        warnings.append(f"Advertencia: La clave 'filterIncludeExtensions' en el bloque {block_idx} fue ignorada por falta de una clave de directorio v√°lida asociada. ‚ö†Ô∏è")
                    
                    # --- RESPONSE_FORMAT (Se registra, no se a√±ade al contexto) ---
                    elif key == "responseformat":
                        if not isinstance(value, str):
                            warnings.append(f"Advertencia: 'responseFormat' en bloque {block_idx} debe ser un string. Ignorado. ‚ö†Ô∏è")
                        # No se a√±ade al final_context_parts, es una directriz para la IA.
                        # En una implementaci√≥n real de Osiris, esta informaci√≥n se pasar√≠a a la configuraci√≥n de la respuesta.
                        # Por ahora, solo lo reconocemos.
                        pass
                    
                    # --- FILE_ENCODING (Ya procesado para block_file_encoding) ---
                    elif key == "fileencoding":
                        pass # Ya procesado al inicio del bloque
                    
                    # --- MAX_CONTEXT_TOKENS (Ya procesado al inicio del bloque) ---
                    elif key == "maxcontexttokens":
                        pass # Ya procesado al inicio del bloque

        except json.JSONDecodeError as e:
            warnings.append(f"Error: El archivo JSON '{resolved_json_path}' est√° mal formado: {e}. Saltando. ‚ùå")
        except Exception as e:
            warnings.append(f"Error inesperado al procesar '{resolved_json_path}': {e}. Saltando. ‚ùå")

    final_context_string = "".join(final_context_parts)
    
    # Advertencia final si el contexto a√∫n se acerca al l√≠mite (considerando que hubo truncamiento anterior)
    if current_total_tokens >= max_tokens_limit * 0.95:
        warnings.append(f"Advertencia final: El contexto generado es muy extenso ({current_total_tokens}/{max_tokens_limit} tokens). La IA podr√≠a tener dificultades. ü•µ")
#    final_context_string = final_context_string.replace("\\n", "\n")
    return str(final_context_string) + str(warnings)


# --- Formateador de Metadata de Path ---
def _format_path_metadata(path, is_directory, base_path_for_relative):
    name = os.path.basename(path) if not is_directory else os.path.basename(path)
    if not name and is_directory: # Caso del directorio ra√≠z de la ruta especificada
        name = os.path.basename(os.path.normpath(path))
        if not name: name = path # Si sigue vac√≠o (ej. para '/'), usar la ruta completa

    file_type = "Directorio" if is_directory else "Archivo"
    extension = os.path.splitext(name)[1] if not is_directory else "N/A"
    
    size_bytes = "N/A"
    if not is_directory and os.path.exists(path) and os.path.isfile(path):
        try:
            size_bytes = os.path.getsize(path)
        except OSError:
            pass # No se pudo obtener el tama√±o

    mod_date = "N/A"
    if os.path.exists(path):
        try:
            mod_timestamp = os.path.getmtime(path)
            mod_date = datetime.fromtimestamp(mod_timestamp).strftime("%Y-%m-%d %H:%M:%S")
        except OSError:
            pass # No se pudo obtener la fecha de modificaci√≥n

    relative_path = os.path.relpath(path, base_path_for_relative) if path.startswith(base_path_for_relative) else path

    metadata_str = (
        f"Nombre: {name}\n"
        f"Tipo: {file_type}\n"
        f"Extension: {extension}\n"
        f"Tama√±o: {size_bytes} bytes\n"
        f"Fecha Modificaci√≥n: {mod_date}\n"
        f"Ruta Relativa al Bloque: {relative_path}\n"
    )
    return metadata_str

# --- Mensaje de Ayuda ---
def _print_help_message():
    print("""
    ‚ú® Ayuda de la Funci√≥n de Inserci√≥n de Fuentes de Proyectos (Osiris Context Loader) ‚ú®
    -----------------------------------------------------------------------------------

    Esta funci√≥n interpreta archivos '.dev.ai.json' para construir un contexto detallado
    para Gemini AI. Los archivos deben ser un arreglo JSON con uno o m√°s objetos,
    cada uno representando un bloque de contexto.

    üìö Estructura B√°sica de un Bloque:
    [
      {
        "human": "Instrucciones del usuario...",
        "aiInstruction": "Directrices para la IA...",
        "metadata": { "project": "MyProject", "version": "1.0" },
        "readFile": ["path/to/file1.js", "path/to/file2.txt"],
        "readDirectoryFiles": "path/to/folder",
        "readDirectoryFilesRecursive": "path/to/recursive_folder",
        "readDirectoryPaths": ["path/to/data_folder"],
        "readDirectoryPathRecursive": ["path/to/all_docs"],
        "filterIncludeExtensions": [".js", ".py"],
        "filterExcludePatterns": ["temp/*.log", "node_modules/**"],
        "maxContextTokens": 500000,
        "responseFormat": "Markdown",
        "fileEncoding": "UTF-8"
      }
    ]

    üîë Claves Admitidas (NO case-sensitive):

    1.  human (string): Texto con instrucciones, explicaciones e indicaciones para la IA.

    2.  aiInstruction (string): Instrucciones operativas directas para la IA sobre c√≥mo procesar
        o priorizar la informaci√≥n del contexto.

    3.  metadata (objeto JSON): Informaci√≥n estructurada sobre el proyecto/componente
        (nombre, versi√≥n, autores, tecnolog√≠as, etc.).

    4.  readFile (arreglo de strings): Rutas a archivos cuyo contenido se leer√° e incluir√°.
        Si un archivo no existe, se avisar√°: "fileRead Path (el path real) Not found".

    5.  readDirectoryFiles (string): Ruta a un directorio. Lee el CONTENIDO de los archivos
        directamente en ese directorio (NO recursivo).

    6.  readDirectoryFilesRecursive (string): Ruta a un directorio. Lee el CONTENIDO de los archivos
        en ese directorio y sus subdirectorios (RECURSIVO).

    7.  readDirectoryPaths (arreglo de strings): Rutas a directorios. No lee el contenido,
        sino que recolecta METADATA de las rutas de archivos/subdirectorios directamente
        en los directorios especificados (NO recursivo).
        Formato de Metadata: Nombre, Tipo, Extensi√≥n, Tama√±o, Fecha Modificaci√≥n, Ruta Relativa al Bloque.

    8.  readDirectoryPathRecursive (arreglo de strings): Rutas a directorios. Recolecta METADATA
        de las rutas de archivos/subdirectorios en esos directorios y sus subdirectorios (RECURSIVO).

    9.  filterIncludeExtensions (arreglo de strings): Extensiones de archivo (ej. ".js", ".py").
        Aplicable a claves de lectura de directorios y paths.
        -   Si ausente o arreglo vac√≠o `[]`: "No aplicar filtro de extensi√≥n" (se consideran todas).
        -   Si contiene extensiones: Solo los archivos con esas extensiones ser√°n considerados.
        -   Ignorada con advertencia si no hay clave de directorio v√°lida asociada.

    10. filterExcludePatterns (arreglo de strings): Rutas o patrones `glob` (ej. "*.log", "node_modules/**").
        Aplicable a claves de lectura de directorios y paths. Los archivos que coincidan
        ser√°n EXCLUIDOS.
        -   Prioridad: `filterExcludePatterns` siempre tiene prioridad sobre `filterIncludeExtensions`.

    11. maxContextTokens (entero): L√≠mite personalizado de tokens para la cadena de contexto.
        Por defecto: 1,000,000 tokens. Si se excede, se intenta truncar o se omiten elementos
        con advertencia.

    12. responseFormat (string): Indica el formato deseado para la respuesta de la IA (ej. "Markdown", "JSON").
        (Esta directriz no se a√±ade al contexto directamente, pero la funci√≥n la reconoce).

    13. fileEncoding (string): Codificaci√≥n de caracteres para los archivos le√≠dos (ej. "UTF-8", "latin-1").
        Por defecto: "UTF-8".

    ‚öôÔ∏è Funcionamiento General:
    -   La funci√≥n construye una √∫nica cadena de texto final con todo el contenido.
    -   Se utilizan marcadores claros (ej. "--- INICIO ARCHIVO: ---") para distinguir las fuentes.
    -   El orden de concatenaci√≥n respeta el orden de las claves en el JSON y el orden de los bloques.
    -   Deduplicaci√≥n Proactiva: Los archivos (contenido o metadata) solo se incluyen una vez.
        Si se intenta incluir un archivo ya procesado, se emite una advertencia y se omite la repetici√≥n.
    -   Gesti√≥n de L√≠mites: Se estima el tama√±o en tokens. Si se acerca al l√≠mite, se advierte.
        Si se excede, se truncan o se omiten elementos (excepto 'human' y 'aiInstruction' que tienen m√°xima prioridad).

    üìù Notas Importantes:
    -   Es tu responsabilidad como humano compositor del JSON evitar configuraciones conflictivas
        que puedan llevar a confusiones l√≥gicas, aunque la deduplicaci√≥n y las advertencias ayudan.
    -   Las rutas relativas se resuelven a partir del directorio donde se encuentra el archivo .dev.ai.json
        o del 'global_base_dir' si se especifica.

    ¬°Usa esta herramienta para darme un contexto rico y estructurado!
    """)



print("LOAD OSIRIS CONTEXT WAS CHARGED")